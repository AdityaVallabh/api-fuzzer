# Execution Plan #

## Summary
This is a living doc on our current execution strategy.

## Plan on architecture
The client side needs to be lean and clean, with no dependencies. The client will be done in golang. Because of the lack of ML and NLP libraries, the client side will only execute well-structured plans, with some tolerence on human edits, but will not do machine learning.

The server side will collect the swagger.json, does NLP analysis on it, and generate the @meqa tags needed for client to execute. 

The testplan.yml can either be generated by the client or the server.

* Client
    * we can have something fully functioning without the server piece. With just people editing the swagger.json.
    * Not sharing test plans between client and server means we don't have a potential breaking point where the two may be out of sync in versions.
    * Our customers can have better performance and flexibility when generating test plans.
    * Can golang client automatically update itself?
* Server
    * We will have more flexibility in the kind of technology we can leverage. For instance, we can leverage spacy.io during plan generation.
    * When the user modifies and overrides some parameters, we need logic to merge the diffs and learn from the user behavior for future test generation.
    * We can more quickly roll out enhancements - when we employ some new algorithms on the server side to generate better plans, all our clients can immediately benefit. We can tell our customers that if they try generate a new plan, the diff will tell them what new tests are added, and they can decide whether to use the new plan or just the old one.
    * The above plan generation on server will be good for SaaS business.

## Language for Test Plan Generation
The language decision depends on whether there is more code overlap between this and the @meqa tag generation, or the golang test-plan execution. 

* @meqa generation
    * Needs to parse swagger.json.
    * Needs to use NLP.
* Test plan generation
    * Needs to parse swagger.json.
    * Needs to have understanding of the test plan structure.
    * Needs to do swagger structure traversal to generate DAG.
    * Needs to use DAG to generate test plan.
    * Needs to understand parameter generation strategies, and understand how to merge newer plans with the customers' modifications on the client side.
    * It seems any NLP related task can be done through @meqa generation step. If needed we can always generate more tags to guide the following steps.
* Test plan execution
    * Needs to parse swagger.json.
    * Needs to have understanding of the test plan structure.
    * Needs to execute test plan.
    * Needs to modify parameters through trial-and-error (future)

Decision

* Test plan generation is done on the server side in golang.
* This gives us the most flexibility in the beginning.
    * We have best leverage of our code base.
    * We have great flexibility when doing customer trials.

## Server

The server has the following pieces

* swagger.json tagging - generating @meqa tags. This is the lowest priority and we can manually generate the tags for now.
* Generating test plan.
* REST server - account management, communicate with the golang client, support UI.

## Test Plan Generation

### Learning from Trello

* Foreign keys - the data model the server implements can have pretty complex data relationships. 
    * Pet belongs to stores
    * Updating a pet with a different store id moves the pet to another store.
    * Now searching/listing for pets in the old store shouldn't show the pet, but in the new store should.
    * The above update is done on the Pet object, but it affects the result of the Store object.
    * What we currently have doesn't factor in foreign keys like above.
    * However, it's easy to add it in. When you have an object that has @meqa[Anotherclass:property] tag for one of its fields, you know it's a foreign key. And we can construct standard tests for foreign keys.
    * We need to be more systematic when constructing test cases. In this case, for PUTs, we need to change one parameter at a time, then validate the change. When changing a foreign key, we need to verify against the foreign table. Similarly during random tests we need to validate against foreign tables too.
* Parameters that mean meta-data within the swagger object structure
    * For instance, /cards/{id}/{field} - the field is the property name for the card. This will return just one property of the card rather than the whole object. We need to have a special tag `<meqa Card.any>`. where any means any field. What if the field can be some but not the others? Later we can enhance it so "any" can be a regexp.
* Object hierarchy
    * Objects can be created and queried based on the hierarchical relationship.
    * POST - boards/{id}/card - create a card that belongs to the specified board.
    * POST - cards, with boardId as argument.
    * GET - boards/{id}/card - get all the cards that belong to the specified board.
    * Likely what we do for foreign keys can sufficiently cover this.
* Incomplete swagger.json
    * for instance, sometimes the example contains more fields than the json object. This we can actually figure out with an ExampleToSchema method. Punt for now unless it becomes a real pain.
* Type confusion
    * Sometimes, an enum is specified as a regular string, with the description saying: valid values are: true, false.
    * We can change the swagger.json to specify the enum properly.
    * Alternatively, we can specify `<meqa enum:true,false>`. This is properly better because it doesn't change the original document, and we know that it won't break anything. Changing the swagger.json to specify enum may break the codegen clients that use the spec.
    * We should design how to use `<meqa>` tags to constrain values. This is very useful when we want to restrict date or number ranges as well.
* Meqa value tags - punt for now, because it's supported by standard swagger tags.
    * `<meqa type:value1,value2,value3>`
    * `<meqa enum:v1,v2,v3>` - note that the swagger.json already has a type for the field.
    * `<meqa range:10,20>` - integer range between 10 and 20.
    * `<meqa range:1.1,102.0>` - float range, again the field has a type in swagger.json

### Test Scenarios

* Opitonal parameters - for APIs that has optional parameters, or objects in parameters that have optional fields, we should vary having and not having the optional stuff.
* We need to support having global parameters. This is useful for
    * api_key
    * things created outside of the scope of the test (e.g. account-name)
* It's not just about covering all the test scenarios. The user should be able to make sense of the test scenarios, be able to search for them, and modify them if needed.
* Process results - it's a common pattern that the real result (object or array) is embedded inside a result object, e.g. {status: success, page: 1, data: [...]}. We need to handle this.
* Sometimes the REST API only exposes part of the functionalities. For instance, the bitbucket REST API doesn't allow user to create commits. Commits can only be done through a git client. To fully test this:
    * The tester needs to use external tools to create some data.
    * The REST api can be used to look at the data created by the external tools (e.g. list all the commits).
    * The result from the above can be used to further popualte the parameters for other tests (e.g. get a specific commit)
    * This means that we can't assert that the data coming from the server exactly matches the client state. We should allow the server to return more. 
    * When there is pagination, we should also expect the server to return less.
* Incomplete swagger.json - this causes a lot of trouble. Static parsing is not possible. Dynamic testing is possible, but it basically requires us to say that the behavior of the server makes sense. This is difficult. Punt for now. We can raise an error instead to let the user know.

### NLP

* Look around
    * Sometimes the comment in one path is not sufficient to determine the meaning of an argument.
    * The same argument name can show up in many places.
    * Other places may have comment that pinpoints the meaning of the argument.
    * We should apply what we figured out to all the places.
* Association
    * The parameters required has strong relationship to the data returned. For instance, when there is an argument called id, and the call returns an User object, most likely the id is for the user.
    * The first step is to create a tool, which will scan a document and apply the same meqa tag to all the variables that's named the same.
    * This can also be done in mqgo. We just need to create a hash table of variable_name->meqa_tag mapping.